{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D энкодер- декодер. Отличие от предыдущей версии - вместо растра точек на вход сети подать вектора лендмарков в виде набора координат и длин.\n",
    "к MDWL добавлены еще 2 лендмарка BCP, FAP - размер энкодера вырос на 3 +3 значения итого 12 значений* 16 зубов = 192."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.datasets import mnist\n",
    "import numpy as np\n",
    "# from jaw_gen import *\n",
    "import tensorflow \n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Dropout\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Lambda\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "# основное отличие - Landmark_gen больше не генерит картинки. только вектора. \n",
    "# и у него теперь есть 3D рисовалка, которая есть независимый метод\n",
    "# генератор теперь в другом файле, генерит дополнительно 2 лендмарка\n",
    "from jaw_gen3d_3lm import Landmark_gen \n",
    "from csv_parser_pd import set_gen_fr_csv_pd\n",
    "# from csv_parser import set_gen_fr_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# готовим СИНТЕТИЧЕСКИЕ входные данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYNT = 0                     # True - учим на синтетике, False - на реальных\n",
    "\n",
    "# готовим СИНТЕТИЧЕСКИЕ входные данные \n",
    "inst_ =                 Landmark_gen()\n",
    "data_len =              5000\n",
    "dataset =               []\n",
    "dataset_spoiled =       []\n",
    "dataset_vec =           []\n",
    "dataset_vec_spoiled =   []\n",
    "\n",
    "if SYNT:\n",
    "    for i in range(data_len):\n",
    "        scale =  9 + np.random.sample()*3     # это для размера 200, диапазон от 14 до 19 для размера 400\n",
    "        factor = 2.2 + np.random.sample()/4   # это для размера 200, диапазон 2.5 - 2.7 для размера 400 \n",
    "        # name = f\"scale {scale:.4}  factor {factor:.3}\"\n",
    "        # сдвиги и генерация остальных лендмарков по умолчанию включены. \n",
    "        vec, vec_spoiled = inst_.image_gen(scale=scale, factor=factor, spoiledMDW=True, shiftMDW=True)\n",
    "\n",
    "        # собираем датасет из векторов \n",
    "        dataset_vec.append(vec)\n",
    "        dataset_vec_spoiled.append(vec_spoiled)\n",
    "    # print (f\"dataset_vec {dataset_vec[:1]}\")\n",
    "    print (f\"dataset_vec len {len(dataset_vec)} \")\n",
    "    print (f\"dataset_vec_spoiled len {len(dataset_vec_spoiled)} \")\n",
    "\n",
    "    # преобразуем датасет в numpy массив\n",
    "    dataset_vec =           np.array(dataset_vec,           dtype=\"float32\") # /2550 тогда не учится ничего \n",
    "    dataset_vec_spoiled =   np.array(dataset_vec_spoiled,   dtype=\"float32\") # /2550\n",
    "\n",
    "# print (f\"datraset vec {dataset_vec[:1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# готовим данные из файла CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data lenght 2398\n"
     ]
    }
   ],
   "source": [
    "# готовим датасет из файла csv\n",
    "# csv_fpath = 'C:/Users/Anton/Projects/jaw_encoder/csv/input_004.csv'\n",
    "# csv_fpath = 'C:/Users/Anton/Projects/jaw_encoder/csv/test10.csv'\n",
    "csv_fpath = 'C:/Users/Anton/Projects/jaw_encoder/csv/input.csv'\n",
    "\n",
    "# выбираем данные из файла\n",
    "if not SYNT:\n",
    "    dataset_vec_spoiled, dataset_vec = set_gen_fr_csv_pd(csv_fpath) # возвращает np массивы сначала T0\n",
    "\n",
    "data_len = len(dataset_vec)\n",
    "print (f\"data lenght {data_len}\")\n",
    "# [print (f\"{i}val {dataset_vec[i][5]} \\n {i}spoil {dataset_vec_spoiled[i][5]}\") for i in range(len(dataset_vec))]\n",
    "# print (f\"val {dataset_vec[3][5]} \\n {dataset_vec_spoiled[3][5]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length vec         2158 - 240\n",
      "length vec spoil   2158 - 240\n"
     ]
    }
   ],
   "source": [
    "# разобьем датасет на тренировочную и тестовую части\n",
    "set_divider_ = round(data_len*0.9)\n",
    "\n",
    "# векторный датасет\n",
    "x_train_vec =   dataset_vec[:set_divider_]\n",
    "x_test_vec =    dataset_vec[set_divider_:]\n",
    "# print (f\"length vec {len(x_train_vec)} - {len(x_test_vec)}\")\n",
    "\n",
    "x_train_vec_spoiled =   dataset_vec_spoiled[:set_divider_]\n",
    "x_test_vec_spoiled =    dataset_vec_spoiled[set_divider_:]\n",
    "# print (f\"vec [0]{x_test_vec[0]}\")\n",
    "print (f\"length vec         {len(x_train_vec)} - {len(x_test_vec)}\")\n",
    "print (f\"length vec spoil   {len(x_train_vec_spoiled)} - {len(x_test_vec_spoiled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Размерности слоев энкодера \n",
    "dense_dim = 192             # размерность полносвязного слоя энкодера 16 зубов * 12 значений.\n",
    "dim_code = dense_dim//6     # размерность кодированного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH_BN = 0 # go with batcNormalization\n",
    "\n",
    "def bn(x):\n",
    "    return BatchNormalization()(x) if WITH_BN else x\n",
    "\n",
    "def create_vec_ae3D():\n",
    "    # 3d кодер - декодер содержит на входе 12*16=192 значений.\n",
    "    activation = 'elu' \n",
    "    # activation = 'linear'\n",
    "    inputs = Input(dense_dim,)\n",
    "\n",
    "    # encoder\n",
    "    x = Dense(dense_dim, activation=activation)(inputs)\n",
    "    x = bn(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = bn(x)\n",
    "    code = Dense(dim_code, activation='linear')(x)\n",
    "\n",
    "    # decoder\n",
    "    input_encoded = Input((dim_code,))\n",
    "    x = Dense(dim_code, activation=activation)(input_encoded)\n",
    "    x = bn(x)\n",
    "    x = Dense(dense_dim, activation=activation)(x)\n",
    "    x = bn(x)\n",
    "    out = Dense(dense_dim, activation='linear')(x)\n",
    "    \n",
    "    # Модели\n",
    "    encoder = Model(inputs, code, name=\"encoder\")\n",
    "    decoder = Model(input_encoded, out, name=\"decoder\")\n",
    "    autoencoder = Model(inputs, decoder(encoder(inputs)), name=\"autoencoder\")\n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "encoder, decoder, autoencoder = create_vec_ae3D()\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# autoencoder.compile(Adam(0.0001), loss='mean_squared_error') \n",
    "\n",
    "# c_autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def create_denoising_model(autoencoder):\n",
    "    \n",
    "    inputs  = Input(batch_shape=(batch_size, dense_dim))\n",
    "    inputs_spoiled  = Input(batch_shape=(batch_size, dense_dim)) # вход для кореженных зубьев\n",
    "\n",
    "    denoiser_model = Model([inputs, inputs_spoiled], autoencoder(inputs_spoiled), name=\"denoiser\")\n",
    "    # return noiser, denoiser_model\n",
    "    return denoiser_model\n",
    "\n",
    "\n",
    "# noiser, denoiser_model = create_denoising_model(c_autoencoder)\n",
    "denoiser_model = create_denoising_model(autoencoder)\n",
    "denoiser_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# denoiser_model.compile(Adam(1e-4), loss='mean_squared_error')\n",
    "# optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3)\n",
    "# keras.optimizers.Adam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digits(*args):\n",
    "    # for a in args:\n",
    "        # print (f\"type args {type(a)}\")\n",
    "    args = [x.squeeze() for x in args]\n",
    "    n = min([x.shape[0] for x in args])\n",
    "    \n",
    "    plt.figure(figsize=(2*n, 2*len(args)))\n",
    "    for j in range(n):\n",
    "            for i in range(len(args)):\n",
    "                ax = plt.subplot(len(args), n, i*n + j + 1)\n",
    "                plt.imshow(args[i][j])\n",
    "                plt.gray()\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss -56.63470458984375\n",
      "Epoch 2 loss -34.01955032348633\n",
      "Epoch 5 loss -17.9257869720459\n",
      "Epoch 10 loss -6.361588478088379\n",
      "Epoch 30 loss -2.8781466484069824\n",
      "Epoch 60 loss -1.2675379514694214\n",
      "Epoch 100 loss -1.5319931507110596\n",
      "Epoch 200 loss -1.1115682125091553\n",
      "Epoch 300 loss -1.035588026046753\n",
      "Epoch 400 loss -0.7879180908203125\n",
      "Epoch 500 loss -0.6476608514785767\n",
      "Epoch 600 loss -0.6896518468856812\n",
      "Epoch 700 loss -0.5236459374427795\n",
      "Epoch 800 loss -0.6977437734603882\n",
      "Epoch 900 loss -0.5533360838890076\n",
      "Epoch 1000 loss -0.41568702459335327\n",
      "Epoch 1100 loss -0.4420456290245056\n",
      "Epoch 1200 loss -0.5509138107299805\n",
      "Epoch 1300 loss -0.37510523200035095\n",
      "Epoch 1400 loss -0.5123335719108582\n",
      "Epoch 1500 loss -0.4170428514480591\n",
      "Epoch 1600 loss -0.39911454916000366\n",
      "Epoch 1700 loss -0.28833574056625366\n",
      "Epoch 1800 loss -0.399850070476532\n",
      "Epoch 1900 loss -0.30286890268325806\n",
      "Epoch 2000 loss -0.2979060411453247\n",
      "Epoch 2100 loss -0.3987938463687897\n",
      "Epoch 2200 loss -0.29423728585243225\n",
      "Epoch 2300 loss -0.3529563844203949\n",
      "Epoch 2400 loss -0.34570685029029846\n",
      "Epoch 2500 loss -0.477181613445282\n",
      "Epoch 2600 loss -0.32137104868888855\n",
      "Epoch 2700 loss -0.32324451208114624\n",
      "Epoch 2800 loss -0.38345348834991455\n",
      "Epoch 2900 loss -0.28742659091949463\n",
      "Epoch 3000 loss -0.3056944012641907\n",
      "Epoch 3100 loss -0.2887326180934906\n",
      "Epoch 3200 loss -0.6223254203796387\n",
      "Epoch 3300 loss -0.2926998436450958\n",
      "Epoch 3400 loss -0.32995468378067017\n",
      "Epoch 3500 loss -0.26796722412109375\n",
      "Epoch 3600 loss -0.30123889446258545\n",
      "Epoch 3700 loss -0.2830106019973755\n",
      "Epoch 3800 loss -0.33097708225250244\n",
      "Epoch 3900 loss -0.24347363412380219\n",
      "Epoch 4000 loss -0.3360191583633423\n",
      "Epoch 4100 loss -0.2553194463253021\n",
      "Epoch 4200 loss -0.24120503664016724\n",
      "Epoch 4300 loss -0.36974430084228516\n",
      "Epoch 4400 loss -0.2501087188720703\n",
      "Epoch 4500 loss -0.3202438950538635\n",
      "Epoch 4600 loss -0.24658125638961792\n",
      "Epoch 4700 loss -0.29625463485717773\n",
      "Epoch 4800 loss -0.233372762799263\n",
      "Epoch 4900 loss -0.27113109827041626\n",
      "Epoch 5000 loss -0.2690221965312958\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 5100 loss -0.33282703161239624\n",
      "Epoch 5200 loss -0.25045132637023926\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 5300 loss -0.2620448172092438\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 5400 loss -0.31574857234954834\n",
      "Epoch 5500 loss -0.1947426050901413\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 5600 loss -0.2075435221195221\n",
      "Epoch 5700 loss -0.23227807879447937\n",
      "Epoch 5800 loss -0.2595294713973999\n",
      "Epoch 5900 loss -0.2127254605293274\n",
      "Epoch 6000 loss -0.364271342754364\n",
      "Epoch 6100 loss -0.1557665318250656\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 6200 loss -0.2602525055408478\n",
      "Epoch 6300 loss -0.22896432876586914\n",
      "Epoch 6400 loss -0.20698553323745728\n",
      "Epoch 6500 loss -0.2236693650484085\n",
      "Epoch 6600 loss -0.2532361149787903\n",
      "Epoch 6700 loss -0.323395311832428\n",
      "Epoch 6800 loss -0.24528293311595917\n",
      "Epoch 6900 loss -0.1917341947555542\n",
      "Epoch 7000 loss -0.19698041677474976\n",
      "Epoch 7100 loss -0.2879657745361328\n",
      "Epoch 7200 loss -0.28770285844802856\n",
      "Epoch 7300 loss -0.23487746715545654\n",
      "Epoch 7400 loss -0.44317686557769775\n",
      "Epoch 7500 loss -0.33356907963752747\n",
      "Epoch 7600 loss -0.3378487229347229\n",
      "Epoch 7700 loss -0.29638609290122986\n",
      "Epoch 7800 loss -0.27444159984588623\n",
      "Epoch 7900 loss -0.2717297375202179\n",
      "Epoch 8000 loss -0.1869276463985443\n",
      "Epoch 8100 loss -0.20221267640590668\n",
      "Epoch 8200 loss -0.20094329118728638\n",
      "Epoch 8300 loss -0.22619017958641052\n",
      "Epoch 8400 loss -0.25734883546829224\n",
      "Epoch 8500 loss -0.2227584719657898\n",
      "Epoch 8600 loss -0.2286183387041092\n",
      "Epoch 8700 loss -0.2215975821018219\n",
      "Epoch 8800 loss -0.3158142566680908\n",
      "Epoch 8900 loss -0.23327644169330597\n",
      "Epoch 9000 loss -0.23755386471748352\n",
      "Epoch 9100 loss -0.19359701871871948\n",
      "Epoch 9200 loss -0.2066611796617508\n",
      "Epoch 9300 loss -0.19767016172409058\n",
      "Epoch 9400 loss -0.2622910439968109\n",
      "Epoch 9500 loss -0.22267119586467743\n",
      "Epoch 9600 loss -0.24045296013355255\n",
      "Epoch 9700 loss -0.23700086772441864\n",
      "Epoch 9800 loss -0.24427205324172974\n",
      "Epoch 9900 loss -0.21898160874843597\n",
      "Epoch 10000 loss -0.16689229011535645\n",
      "Epoch 10100 loss -0.19638603925704956\n",
      "Epoch 10200 loss -0.20494697988033295\n",
      "Epoch 10300 loss -0.19423294067382812\n",
      "Epoch 10400 loss -0.7150065898895264\n",
      "Epoch 10500 loss -0.17766013741493225\n",
      "Epoch 10600 loss -0.18174241483211517\n",
      "Epoch 10700 loss -0.24226298928260803\n",
      "Epoch 10800 loss -0.265116423368454\n",
      "Epoch 10900 loss -0.1778019368648529\n",
      "Epoch 11000 loss -0.21565306186676025\n",
      "Epoch 11100 loss -0.21086037158966064\n",
      "Epoch 11200 loss -0.2072894126176834\n",
      "Epoch 11300 loss -0.18393225967884064\n",
      "Epoch 11400 loss -0.1858387589454651\n",
      "Epoch 11500 loss -0.25536277890205383\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 11600 loss -0.1552431881427765\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 11700 loss -0.20090045034885406\n",
      "Epoch 11800 loss -0.1725577414035797\n",
      "Epoch 11900 loss -0.1657758206129074\n",
      "Epoch 12000 loss -0.21192002296447754\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 12100 loss -0.17178726196289062\n",
      "Epoch 12200 loss -0.2313767373561859\n",
      "Epoch 12300 loss -0.1870148926973343\n",
      "Epoch 12400 loss -0.20530234277248383\n",
      "Epoch 12500 loss -0.17181099951267242\n",
      "Epoch 12600 loss -0.1815464347600937\n",
      "Epoch 12700 loss -0.7681087255477905\n",
      "Epoch 12800 loss -0.16910719871520996\n",
      "Epoch 12900 loss -0.16752761602401733\n",
      "Epoch 13000 loss -0.20256775617599487\n",
      "Epoch 13100 loss -0.183121919631958\n",
      "Epoch 13200 loss -0.2190965712070465\n",
      "Epoch 13300 loss -0.16321596503257751\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 13400 loss -0.1785927414894104\n",
      "Epoch 13500 loss -0.19576826691627502\n",
      "Epoch 13600 loss -1.5202151536941528\n",
      "Epoch 13700 loss -0.231754332780838\n",
      "Epoch 13800 loss -0.17212679982185364\n",
      "Epoch 13900 loss -0.1890631914138794\n",
      "Epoch 14000 loss -0.23315317928791046\n",
      "Epoch 14100 loss -0.17339405417442322\n",
      "Epoch 14200 loss -0.15817785263061523\n",
      "Epoch 14300 loss -0.17181503772735596\n",
      "Epoch 14400 loss -0.19818377494812012\n",
      "Epoch 14500 loss -0.1920415759086609\n",
      "Epoch 14600 loss -0.17199720442295074\n",
      "Epoch 14700 loss -0.18436798453330994\n",
      "Epoch 14800 loss -0.14992564916610718\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 14900 loss -0.1915166676044464\n",
      "Epoch 15000 loss -0.19278693199157715\n",
      "Epoch 15100 loss -0.23091596364974976\n",
      "Epoch 15200 loss -0.17184457182884216\n",
      "Epoch 15300 loss -0.1949680745601654\n",
      "Epoch 15400 loss -0.1559474766254425\n",
      "Epoch 15500 loss -0.2374236136674881\n",
      "Epoch 15600 loss -0.17592374980449677\n",
      "Epoch 15700 loss -0.19116364419460297\n",
      "Epoch 15800 loss -0.21237298846244812\n",
      "Epoch 15900 loss -0.16278602182865143\n",
      "Epoch 16000 loss -0.16668474674224854\n",
      "Epoch 16100 loss -0.21384522318840027\n",
      "Epoch 16200 loss -0.18176646530628204\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Epoch 16300 loss -0.1688472330570221\n",
      "Epoch 16400 loss -0.17301711440086365\n",
      "Epoch 16500 loss -0.14777955412864685\n",
      "Epoch 16600 loss -0.1592046022415161\n",
      "Epoch 16700 loss -0.16679775714874268\n",
      "Epoch 16800 loss -0.1601792275905609\n",
      "Epoch 16900 loss -0.18314632773399353\n",
      "Epoch 17000 loss -0.1705188751220703\n",
      "Epoch 17100 loss -0.2026151418685913\n",
      "Epoch 17200 loss -0.15233445167541504\n",
      "Epoch 17300 loss -0.1940450668334961\n",
      "Epoch 17400 loss -0.21765516698360443\n",
      "Epoch 17500 loss -0.2579647898674011\n",
      "Epoch 17600 loss -0.18566909432411194\n",
      "Epoch 17700 loss -0.1705942451953888\n",
      "Epoch 17800 loss -0.15982899069786072\n",
      "Epoch 17900 loss -0.1517193615436554\n",
      "Epoch 18000 loss -0.787855863571167\n",
      "Epoch 18100 loss -0.3057551383972168\n",
      "Epoch 18200 loss -0.253456711769104\n",
      "Epoch 18300 loss -0.562217652797699\n",
      "Epoch 18400 loss -0.21724343299865723\n",
      "Epoch 18500 loss -0.19699221849441528\n",
      "Epoch 18600 loss -0.21410050988197327\n",
      "Epoch 18700 loss -0.23620522022247314\n",
      "Epoch 18800 loss -0.27669504284858704\n",
      "Epoch 18900 loss -0.2057507485151291\n",
      "Epoch 19000 loss -0.19697953760623932\n",
      "Epoch 19100 loss -0.18412898480892181\n",
      "Epoch 19200 loss -0.20871511101722717\n",
      "Epoch 19300 loss -0.20523184537887573\n",
      "Epoch 19400 loss -0.16875721514225006\n",
      "Epoch 19500 loss -0.1664837896823883\n",
      "Epoch 19600 loss -0.19447720050811768\n",
      "Epoch 19700 loss -0.16489341855049133\n",
      "Epoch 19800 loss -0.21655666828155518\n",
      "Epoch 19900 loss -0.2032260149717331\n",
      "Epoch 20000 loss -0.18439674377441406\n"
     ]
    }
   ],
   "source": [
    "# вместо фита будем делать  model.train_on_batch читобы смотреть как учится и куда движемся \n",
    "# хотя у фита много больше возможностей с колбеками\n",
    "def train_on_batch(vec, vec_spoil, batch_size=batch_size, epochs=100):\n",
    "    # подровняем массив картинок и обучающих векторов так, чтоб np.reshape не ругался - на несовпадение размерности\n",
    "    # сделаем его длину кратной размеру батча\n",
    "    \n",
    "    vec =   vec [:len(x_train_vec)//batch_size*batch_size]\n",
    "    vec =   np.reshape(vec, (-1, batch_size, dense_dim))           # x_test  = np.reshape(x_test,  (len(x_test),  200, 200, 1))\n",
    "    \n",
    "    # и массив кореженных тоже\n",
    "    vec_spoil = vec_spoil   [:len(x_train_vec_spoiled)  //batch_size*batch_size]\n",
    "    vec_spoil = np.reshape(vec_spoil,   (-1, batch_size, dense_dim)) \n",
    "\n",
    "    min_loss = 10e100 # потери для сравнения эпох будем тут сохранять \n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        for i, _ in enumerate(vec):\n",
    "            batch_vec =         vec[i] \n",
    "            batch_vec_spoiled = vec_spoil[i]\n",
    "\n",
    "            loss = denoiser_model.train_on_batch([batch_vec, batch_vec_spoiled], batch_vec)\n",
    "\n",
    "        # эпоха отучилась, построим что вышло.\n",
    "        n = 10\n",
    "        vecs = batch_vec                    # это последний батч в каждой эпохе.\n",
    "        # print (f\"1! vecs shape{vecs.shape}\") # shape - (256, 64)\n",
    "        noised_vecs = batch_vec_spoiled     \n",
    "        # а картинки не участвуют в учении, количество бачей у них при делении датасета на бачи\n",
    "        # с векторами разное, поэтому найти соответствующую картинку проблематично. \n",
    "        # noised_imgs = noiser.predict(imgs, batch_size=batch_size)\n",
    "\n",
    "        # задача в том, чтобы картинке сопоставить спойленую картинку ( это не сложно из сгенерированной \n",
    "        # выборки сделать), но потом предиктить для этой картинки вектора. и рисовать их на новой картинке \n",
    "        # средствами opencv \n",
    "        encoded_vecs = encoder.predict(noised_vecs[:n],  batch_size=n)\n",
    "        decoded_vecs = decoder.predict(encoded_vecs[:n], batch_size=n)\n",
    "\n",
    "        # формируем картинки из предсказанных векторов\n",
    "        vecs =              np.reshape(vecs,            (-1, 16, dense_dim//16))\n",
    "        noised_vecs =       np.reshape(noised_vecs,     (-1, 16, dense_dim//16))\n",
    "        predicted_vecs =    np.reshape(decoded_vecs,    (-1, 16, dense_dim//16))\n",
    "        \n",
    "        # print (f\"vecs shape{vecs.shape} vecs[0] {vecs[0]}\")\n",
    "        if (epoch%100 ==0) or (epoch in [1,2, 5, 10, 30, 60]):\n",
    "            print(f\"Epoch {epoch} loss -{loss}\")\n",
    "        # рисуем картинки и сохраняем модели\n",
    "        if (epoch % 50 ==0) and (epoch >= 5000):\n",
    "            # print(f'Epoch {epoch}')\n",
    "            # plot_digits(batch_x[:n], batch_x_spoil[:n], imgs_fr_vec[:n], imgs_spoil_fr_vec[:n], imgs_fr_decoded_vec[:n])            \n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                # save only the best models\n",
    "                #denoiser_model.save(f'models3D/denoiser{epoch}ep_loss__{loss:.3}__.h5')\n",
    "                decoder.save(f'models3D/decoder{epoch}ep_{data_len}len_{batch_size}btsz_bn{WITH_BN}__loss__{loss:.3}__.h5')\n",
    "                encoder.save(f'models3D/encoder{epoch}ep_{data_len}len_{batch_size}btsz_bn{WITH_BN}__loss__{loss:.3}__.h5')\n",
    "        \n",
    "train_on_batch(x_train_vec, x_train_vec_spoiled, batch_size=batch_size, epochs=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_autoencoder.save(f'models/c_autoencoder.h5'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04635d289a519a1410467dd0afb0db42f9184808881ca68b2eb5a687a20a5a94"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
